---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hello! I‚Äôm **Yao** **Zhang** (Âº†Áë∂). I am currently a lecturer at the School of Statistics and Data Science, Nankai University.

Before that, I received my Ph.D. (2022) and B.S. (2017) from Nankai University, advised by [Prof. Zhenglu Yang](https://scholar.google.com/citations?user=u5LzNHcAAAAJ&hl=zh-CN). From 2021 to 2022, I was a visiting student at the [NExT](https://www.nextcenter.org/) Research Centre at the National University of Singapore (NUS), where I was mentored by [Prof. Wenqiang Lei](https://scholar.google.com/citations?user=qexdxuEAAAAJ&hl=zh-CN) and [Prof. Chua Tat-Seng](https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=zh-CN).

My primary research interests lie in the area of Statistics and Data Science, with a focus on the following topics:
Knowledge Reasoning; 
Medical NLP; 
Safe AI.



# üî• News
- *2025.04*: &nbsp;üéâüéâüéâ Three papers are accepted by ACL 2025. 

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/ding25acl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generating Questions, Answers, and Distractors for Videos: Exploring Semantic Uncertainty of Object Motions](https://aclanthology.org/2025.findings-acl.376/)

Wenjian Ding, **Yao Zhang**#, Jun Wang, Adam Jatowt, Zhenglu Yang#

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/qin25acl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Listening to Patients: A Framework of Detecting and Mitigating Patient Misreport for Medical Dialogue Generation](https://aclanthology.org/2025.findings-acl.135/)

Lang Qin, **Yao Zhang**#, Hongru Liang, Adam Jatowt, Zhenglu Yang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025</div><img src='images/xiao25acl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View](https://aclanthology.org/2025.acl-long.852/)

Yongjie Xiao, Hongru Liang, Peixin Qin, **Yao Zhang**, Wenqiang Lei

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">DASFAA 2025</div><img src='images/bao25dasfaa.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Enhancing Cross-Lingual Dialogue Summarization through Interpretable Chain-of-Thought](https://dasfaa2025.github.io/#/program/research#short-papers)

Zhongtian Bao, **Yao Zhang**, Jun Wang, Adam Jatowt, and Zhenglu Yang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">COLING 2024</div><img src='images/ding24coling.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Can we learn question, answer, and distractors all from an image? a new task for multiple-choice visual question answering](https://aclanthology.org/2024.lrec-main.254/)

Wenjian Ding, **Yao Zhang**, Jun Wang, Adam Jatowt, and Zhenglu Yang#

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2024</div><img src='images/dingemnlp24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors](https://aclanthology.org/2024.emnlp-main.88/)

Wenjian Ding, **Yao Zhang**, Jun Wang, Adam Jatowt, and Zhenglu Yang#

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2024</div><img src='images/qin23emnlp.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Well begun is half done: Generator-agnostic knowledge pre-selection for knowledge-grounded dialogue](https://aclanthology.org/2023.emnlp-main.285/)

Lang Qin‚àó,**Yao Zhang**#, Hongru Liang, Jun Wang, and Zhenglu Yang#

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2024</div><img src='images/qin23emnlp.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Well begun is half done: Generator-agnostic knowledge pre-selection for knowledge-grounded dialogue](https://aclanthology.org/2023.emnlp-main.285/)

Lang Qin‚àó,**Yao Zhang**#, Hongru Liang, Jun Wang, and Zhenglu Yang#

</div>
</div>






